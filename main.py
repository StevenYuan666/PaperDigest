import os
import requests
import datetime
import arxiv
from openai import OpenAI

# --- é…ç½®éƒ¨åˆ† ---
API_KEY = os.getenv("THIRD_PARTY_API_KEY") 
BASE_URL = "https://endpoint.greatrouter.com" 
MODEL_NAME = "gpt-5-nano"

# Discord é…ç½®
WEBHOOK_URL = os.getenv("DISCORD_WEBHOOK_URL") 

# ä½ çš„å…³é”®è¯
KEYWORDS = [
    "Agents", 
    "Large Language Models", 
    "Vision Language Models", 
    "LLM Personalization", 
    "RAG", 
    "Reasoning", 
    "Latent Reasoning"
]

client = OpenAI(api_key=API_KEY, base_url=BASE_URL)

# --- å·¥å…·å‡½æ•° ---

def get_arxiv_papers_by_keywords():
    """
    é’ˆå¯¹æ¯ä¸ªå…³é”®è¯å•ç‹¬æœç´¢ï¼Œä¸é™åˆ¶åˆ†ç±» (cs.CL, cs.LG, cs.CV, cs.AI å‡å¯è¢«æœåˆ°)ã€‚
    """
    print("æ­£åœ¨æ ¹æ®å…³é”®è¯é€ä¸ªæŠ“å– Arxiv (Global Search)...")
    
    arxiv_client = arxiv.Client()
    collected_papers = []
    seen_ids = set() 

    for keyword in KEYWORDS:
        print(f"  > æ­£åœ¨æœç´¢: {keyword} ...")
        
        # --- æ ¸å¿ƒä¿®æ”¹ ---
        # æ—§é€»è¾‘: f'cat:cs.CL AND (ti:"{keyword}" OR abs:"{keyword}")'
        # æ–°é€»è¾‘: åªè¦æ ‡é¢˜æˆ–æ‘˜è¦åŒ…å«å…³é”®è¯å³å¯ï¼Œä¸é™ Category
        query = f'ti:"{keyword}" OR abs:"{keyword}"'
        
        search = arxiv.Search(
            query=query,
            max_results=5, 
            sort_by=arxiv.SortCriterion.SubmittedDate
        )
        
        found_for_this_keyword = False
        
        for result in arxiv_client.results(search):
            # 1. æ£€æŸ¥å»é‡
            if result.entry_id in seen_ids:
                continue
            
            # 2. æ£€æŸ¥æ—¶é—´ï¼ˆæœ€è¿‘ 48 å°æ—¶ï¼‰
            if (datetime.datetime.now(datetime.timezone.utc) - result.published).days > 2:
                continue
            
            # 3. (å¯é€‰) ç®€å•çš„å™ªå£°è¿‡æ»¤
            # è™½ç„¶æˆ‘ä»¬æ”¾å¼€äº†åˆ†ç±»ï¼Œä½†ä¸ºäº†é˜²æ­¢ "Agents" æœåˆ°çº¯ç»æµå­¦è®ºæ–‡ï¼Œ
            # å¯ä»¥æ£€æŸ¥ä¸€ä¸‹ primary_category æ˜¯å¦å±äºè®¡ç®—æœºæˆ–ç»Ÿè®¡å­¦ (cs.*, stat.*)
            # å¦‚æœä½ æƒ³è¦æœ€å¤§èŒƒå›´ï¼Œå¯ä»¥æŠŠä¸‹é¢è¿™ä¸¤è¡Œæ³¨é‡Šæ‰
            if not result.primary_category.startswith(('cs', 'stat')):
                 continue 

            collected_papers.append({
                "source": f"Arxiv [{result.primary_category}]", # æ˜¾ç¤ºå…·ä½“åˆ†ç±»ï¼Œæ–¹ä¾¿ä½ ç¡®è®¤æ¥æº
                "title": result.title,
                "url": result.entry_id,
                "abstract": result.summary,
                "authors": ", ".join([a.name for a in result.authors[:3]]) + " et al.",
                "color": 16711680 # çº¢è‰²
            })
            
            seen_ids.add(result.entry_id)
            found_for_this_keyword = True
            break # æ‰¾åˆ°ä¸€ç¯‡æœ€æ–°çš„å°±è·³åˆ°ä¸‹ä¸€ä¸ªå…³é”®è¯
        
        if not found_for_this_keyword:
            print(f"    - {keyword}: æš‚æ— æœ€æ–°ç›¸å…³è®ºæ–‡")

    return collected_papers

def get_huggingface_daily_papers(max_results=2):
    """è·å– Hugging Face çƒ­é—¨"""
    print("æ­£åœ¨æŠ“å– Hugging Face Daily Papers...")
    today = datetime.datetime.now().strftime("%Y-%m-%d")
    url = f"https://huggingface.co/api/daily_papers?date={today}"
    
    try:
        resp = requests.get(url)
        # è‡ªåŠ¨å›é€€æ—¥æœŸé€»è¾‘
        if resp.status_code != 200 or not resp.json():
            yesterday = (datetime.datetime.now() - datetime.timedelta(days=1)).strftime("%Y-%m-%d")
            url = f"https://huggingface.co/api/daily_papers?date={yesterday}"
            resp = requests.get(url)
            
        data = resp.json()
        if not isinstance(data, list): return []

        sorted_papers = sorted(data, key=lambda x: x.get('upvotes', 0), reverse=True)[:max_results]
        
        papers = []
        for p in sorted_papers:
            paper_info = p.get('paper', {})
            if not paper_info: continue
            
            papers.append({
                "source": "Hugging Face ğŸ”¥",
                "title": paper_info.get('title', 'Unknown'),
                "url": f"https://huggingface.co/papers/{paper_info.get('id', '')}",
                "abstract": paper_info.get('summary', ''), 
                "authors": "Community Trending", 
                "color": 16776960 # é»„è‰²
            })
        return papers
    except Exception as e:
        print(f"HFæŠ“å–å¤±è´¥: {e}")
        return []

def summarize_with_ai(paper_data):
    """è°ƒç”¨ API æ€»ç»“"""
    prompt = f"""
    You are a research assistant. Summarize this paper for an expert.
    
    Paper Title: {paper_data['title']}
    Abstract: {paper_data['abstract']}
    
    Format output strictly:
    **TL;DR**: [One concise sentence]
    **Key Innovation**: [1-2 bullet points on technical novelty]
    **Performance**: [Main metric or result if available]
    """
    
    try:
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Summary failed: {str(e)}"

def send_discord_embed(paper_data, summary):
    """å‘é€ Discord Embed"""
    embed = {
        "title": paper_data['title'],
        "url": paper_data['url'],
        "description": summary,
        "color": paper_data['color'],
        "fields": [
            {
                "name": "Topic / Source",
                "value": paper_data['source'],
                "inline": True
            },
            {
                "name": "Authors",
                "value": paper_data['authors'],
                "inline": True
            }
        ],
        "footer": {
            "text": f"Generated by {MODEL_NAME} â€¢ {datetime.datetime.now().strftime('%Y-%m-%d')}"
        }
    }
    
    requests.post(WEBHOOK_URL, json={"embeds": [embed]})

# --- ä¸»ç¨‹åº ---
if __name__ == "__main__":
    all_papers = []
    
    # 1. æŠ“å– Arxiv (å…¨åº“)
    try:
        all_papers.extend(get_arxiv_papers_by_keywords())
    except Exception as e:
        print(f"Arxiv æ¨¡å—å‡ºé”™: {e}")
    
    # 2. æŠ“å– HF (çƒ­é—¨è¡¥å……)
    hf_papers = get_huggingface_daily_papers(max_results=3)
    
    # å»é‡
    existing_titles = {p['title'].lower() for p in all_papers}
    for hf in hf_papers:
        if hf['title'].lower() not in existing_titles:
            all_papers.append(hf)
    
    print(f"å…±è·å–åˆ° {len(all_papers)} ç¯‡è®ºæ–‡ï¼Œå¼€å§‹æ€»ç»“å‘é€...")
    
    for paper in all_papers:
        print(f"å¤„ç†: {paper['title']}")
        summary = summarize_with_ai(paper)
        send_discord_embed(paper, summary)
        
    print("å…¨éƒ¨å®Œæˆï¼")